{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447777ac362a3ac2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Text pair classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba7732a8ce25115d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T14:51:30.456562500Z",
     "start_time": "2024-11-16T14:51:30.378738900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "OUTPUT_DIR = Path(\"./output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a656fbbb9fde4a3f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tasks 1, 2 - Load and prepare data\n",
    "\n",
    "Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6d632d370b44b9b0"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T14:51:36.753970300Z",
     "start_time": "2024-11-16T14:51:30.383561800Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "fiqa_corpus = load_dataset(\"clarin-knext/fiqa-pl\", \"corpus\")[\"corpus\"]\n",
    "fiqa_queries = load_dataset(\"clarin-knext/fiqa-pl\", \"queries\")[\"queries\"]\n",
    "fiqa_qa = load_dataset(\"clarin-knext/fiqa-pl-qrels\")[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ea1ff2e81d9c69",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create a map query ID -> set of matching corpus IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ecf884419d7e27f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T14:51:36.780933600Z",
     "start_time": "2024-11-16T14:51:36.753970300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qc_map = {}\n",
    "for q_id, c_id in zip(fiqa_qa[\"query-id\"], fiqa_qa[\"corpus-id\"]):\n",
    "    if q_id not in qc_map:\n",
    "        qc_map[q_id] = set()\n",
    "    qc_map[q_id].add(c_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bcb8ca6e129942",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create a map of ID -> text for easier processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70e1d2e90c3398e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T14:51:36.978798800Z",
     "start_time": "2024-11-16T14:51:36.779934500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_map = {int(idx): q for idx, q in zip(fiqa_corpus[\"_id\"], fiqa_corpus[\"text\"])}\n",
    "query_map = {int(idx): q for idx, q in zip(fiqa_queries[\"_id\"], fiqa_queries[\"text\"])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72476a9b6ae92759",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd5c0179692bd4e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create positive pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b58f9ab0dc2178f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T14:51:37.000563800Z",
     "start_time": "2024-11-16T14:51:36.979799100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_POSITIVES = 400\n",
    "NEG_POS_RATIO = 2\n",
    "NUM_NEGATIVES = int(NUM_POSITIVES * NEG_POS_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988bf70e66f1ed29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T14:51:37.000563800Z",
     "start_time": "2024-11-16T14:51:36.983158800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_queries = list(qc_map.keys())[:NUM_POSITIVES]\n",
    "positive_answers = [list(qc_map[q_id])[0] for q_id in positive_queries]\n",
    "\n",
    "data_pos = list(zip(positive_queries, positive_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a51a07030c9d9edc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T14:51:37.000563800Z",
     "start_time": "2024-11-16T14:51:36.987034300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co jest uważane za wydatek służbowy w podróży służbowej?\n",
      "\n",
      "Wytyczne IRS dotyczące tematu. Ogólnie rzecz biorąc, najlepsze, co mogę powiedzieć, to to, że Twój wydatek biznesowy może podlegać odliczeniu. Ale to zależy od okoliczności i tego, co chcesz odliczyć. Podróże Podatnicy, którzy wyjeżdżają z domu w celach służbowych, mogą odliczyć związane z tym wydatki, w tym koszty dotarcia do miejsca docelowego, koszty zakwaterowania i wyżywienia oraz inne zwykłe i niezbędne wydatki. Podatnicy są uważani za „wyjeżdżających poza dom”, jeśli ich obowiązki wymagają od nich przebywania poza domem znacznie dłużej niż zwykły dzień pracy i muszą spać lub odpoczywać, aby sprostać wymogom pracy. Można odliczyć rzeczywisty koszt posiłków i nieprzewidziane wydatki lub skorzystać ze standardowej diety żywieniowej i obniżonych wymogów ewidencji. Niezależnie od zastosowanej metody odliczenia posiłków są zazwyczaj ograniczone do 50 procent, jak wspomniano wcześniej. Jako koszt można zgłaszać tylko rzeczywiste koszty zakwaterowania, a rachunki należy przechowywać do dokumentacji. Wydatki muszą być rozsądne i odpowiednie; potrącenia z tytułu nadmiernych wydatków nie są dopuszczalne. Więcej informacji można znaleźć w publikacji 463, Podróże, rozrywka, prezenty i wydatki na samochód. Rozrywka Wydatki na rozrywkę dla klientów, klientów lub pracowników mogą być odliczane, jeśli są one zarówno zwyczajne, jak i konieczne oraz spełniają jeden z następujących testów: Test bezpośrednio związany: Głównym celem działalności rozrywkowej jest prowadzenie działalności, działalność była faktycznie prowadzona podczas działalność i podatnik mieli więcej niż ogólne oczekiwanie uzyskania dochodu lub innej konkretnej korzyści biznesowej w przyszłości. Powiązany test: rozrywka była związana z aktywnym prowadzeniem działalności handlowej lub biznesowej podatnika i miała miejsce bezpośrednio przed lub po istotnej dyskusji biznesowej. Publikacja 463 zawiera obszerniejsze wyjaśnienie tych testów, jak również innych ograniczeń i wymagań dotyczących odliczania wydatków na rozrywkę. Prezenty Podatnicy mogą odliczyć część lub całość kosztów prezentów wręczanych w ramach ich działalności handlowej lub biznesowej. Ogólnie rzecz biorąc, odliczenie jest ograniczone do 25 USD za prezenty wręczane bezpośrednio lub pośrednio jednej osobie w ciągu roku podatkowego. Więcej omówienia zasad i ograniczeń można znaleźć w Publikacji 463. Jeśli Twoja spółka LLC zwróci Ci wydatki wykraczające poza te wytyczne, należy je traktować jako dochód dla celów podatkowych. Edytuj koszty posiłków: Kwota standardowego dodatku na posiłki. Standardowy dodatek na posiłki to federalna stawka M&IE. W przypadku podróży w 2010 r. stawka dla większości małych miejscowości w Stanach Zjednoczonych wynosi 46 USD dziennie. Źródło IRS P463 Alternatywnie możesz dokonać zwrotu według stawki dziennej\n"
     ]
    }
   ],
   "source": [
    "print(query_map[data_pos[0][0]], end=\"\\n\\n\")\n",
    "print(corpus_map[data_pos[0][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b36085861917925",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d806ff507961dc84",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create negative pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "838a24dda3502d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T14:51:38.489963900Z",
     "start_time": "2024-11-16T14:51:36.990563Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from random import seed\n",
    "\n",
    "seed(42)\n",
    "\n",
    "# don't use the same queries as in positive pairs\n",
    "negative_queries = list(qc_map.keys() - positive_queries)[:NUM_NEGATIVES]\n",
    "negative_answers = []\n",
    "for q_id in negative_queries:\n",
    "    # sample one answer not present in \"correct answers\" or in positive answers defined above\n",
    "    potential_negative_answers = sorted(\n",
    "        corpus_map.keys() - (qc_map[q_id] | set(positive_answers))\n",
    "    )\n",
    "    negative_answers.append(sample(potential_negative_answers, 1)[0])\n",
    "\n",
    "data_neg = list(zip(negative_queries, negative_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1de7eb10825305b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T14:51:38.504230400Z",
     "start_time": "2024-11-16T14:51:38.490964200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precedens i modele 100% kapitału dostępne w ramach pierwszej oferty?\n",
      "\n",
      "W przypadku eToro, tak jak w przypadku każdej innej firmy maklerskiej, możesz stracić cały swój kapitał. Proponuję zainwestować w jeden lub więcej funduszy giełdowych, które śledzą główne indeksy. Jeśli nie, po prostu włóż pieniądze na stałe konta depozytowe; zyskaj trochę zainteresowania i załóż najpierw fundusz awaryjny, zanim zainwestujesz pieniądze, które czujesz, że jesteś w stanie stracić.\n"
     ]
    }
   ],
   "source": [
    "print(query_map[data_neg[0][0]], end=\"\\n\\n\")\n",
    "print(corpus_map[data_neg[0][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1088f25f3a619d71",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e880370e26ef7469",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 3 - Split dataset into train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d8c4641765270dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T14:51:38.857248900Z",
     "start_time": "2024-11-16T14:51:38.493231300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_pos, test_pos = train_test_split(data_pos, train_size=0.7, random_state=42)\n",
    "valid_pos, test_pos = train_test_split(test_pos, train_size=0.5, random_state=42)\n",
    "\n",
    "train_neg, test_neg = train_test_split(data_neg, train_size=0.7, random_state=42)\n",
    "valid_neg, test_neg = train_test_split(test_neg, train_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd114a1ef1922c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "Convert to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "137a68ff104825e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T14:51:38.870808300Z",
     "start_time": "2024-11-16T14:51:38.857248900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert(q, a, sep):\n",
    "    return query_map[q] + f\" {sep} \" + corpus_map[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f15ac69baaabf4dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T14:51:38.870808300Z",
     "start_time": "2024-11-16T14:51:38.862056300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_pos_text = [{\"text\": convert(q, a, \"[SEP]\"), \"labels\": 1} for q, a in train_pos]\n",
    "train_neg_text = [{\"text\": convert(q, a, \"[SEP]\"), \"labels\": 0} for q, a in train_neg]\n",
    "\n",
    "valid_pos_text = [{\"text\": convert(q, a, \"[SEP]\"), \"labels\": 1} for q, a in valid_pos]\n",
    "valid_neg_text = [{\"text\": convert(q, a, \"[SEP]\"), \"labels\": 0} for q, a in valid_neg]\n",
    "\n",
    "test_pos_text = [{\"text\": convert(q, a, \"[SEP]\"), \"labels\": 1} for q, a in test_pos]\n",
    "test_neg_text = [{\"text\": convert(q, a, \"[SEP]\"), \"labels\": 0} for q, a in test_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a908150be64a17ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T14:51:38.903011100Z",
     "start_time": "2024-11-16T14:51:38.866807100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_list(train_pos_text + train_neg_text)\n",
    "valid_dataset = Dataset.from_list(valid_pos_text + valid_neg_text)\n",
    "test_dataset = Dataset.from_list(test_pos_text + test_neg_text)\n",
    "\n",
    "dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": valid_dataset,\n",
    "        \"test\": test_dataset,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337d90f5de0b8d12",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1746c64c23173f2a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 4 - Train a model\n",
    "\n",
    "Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d5389de774c9da3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T14:51:40.322042700Z",
     "start_time": "2024-11-16T14:51:38.882012Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35ed972e-a87a-4968-9c0d-31eee11e643d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3090'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d18c99eeeba0c9af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T14:57:57.686584200Z",
     "start_time": "2024-11-16T14:57:56.129515700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75999b4c30844c7aa1437e3d0d31b56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728f46105def4134af9f4c07b951ec33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8cc854bf2e94f3c995af66324bbb51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def preprocess_function(data):\n",
    "    return tokenizer(\n",
    "        data[\"text\"], padding=\"max_length\", truncation=True, max_length=128\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a1592c805b855c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T14:57:58.736970100Z",
     "start_time": "2024-11-16T14:57:57.686584200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb5b21d57d979d44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T15:27:53.288223900Z",
     "start_time": "2024-11-16T15:27:53.239543400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = OUTPUT_DIR / \"results\"\n",
    "log_dir = OUTPUT_DIR / \"logs\"\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "log_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1.2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.02,\n",
    "    logging_dir=log_dir,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"auroc\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76a07d4-9243-48d4-bc01-4c6e6c3d74e0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82216eff-79e2-4353-8b80-cac8fae887a5",
   "metadata": {},
   "source": [
    "### Task 5 - Monitor the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d4c887b143ed176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T15:27:53.934115700Z",
     "start_time": "2024-11-16T15:27:53.920675600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    predictions = pred.predictions.argmax(-1)\n",
    "    probabilities = pred.predictions\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "    try:\n",
    "        auroc = roc_auc_score(labels, probabilities[:, 1])\n",
    "    except ValueError:\n",
    "        auroc = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"auroc\": auroc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c078eccd24d1e1c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T15:28:03.142033900Z",
     "start_time": "2024-11-16T15:27:54.424231300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\piotr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1350' max='1350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1350/1350 04:54, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Auroc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.642400</td>\n",
       "      <td>0.633190</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.594306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.625300</td>\n",
       "      <td>0.615185</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.641944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.597700</td>\n",
       "      <td>0.608354</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.685972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.546900</td>\n",
       "      <td>0.539692</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.786250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.531500</td>\n",
       "      <td>0.508035</td>\n",
       "      <td>0.727778</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.575342</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.820833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.355200</td>\n",
       "      <td>0.466881</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>0.647619</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.846528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.513540</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.671533</td>\n",
       "      <td>0.597403</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.842222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.299500</td>\n",
       "      <td>0.549685</td>\n",
       "      <td>0.744444</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.842778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.168500</td>\n",
       "      <td>0.570313</td>\n",
       "      <td>0.738889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.580247</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.845139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.142900</td>\n",
       "      <td>0.700745</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.827639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.124000</td>\n",
       "      <td>0.701657</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.797222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.108200</td>\n",
       "      <td>1.019391</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.635762</td>\n",
       "      <td>0.527473</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.829861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.841134</td>\n",
       "      <td>0.772222</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.650794</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.819028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>1.010905</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.635514</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.777639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>1.064038</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.789583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>1.107448</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>0.654206</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.757083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>1.295527</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.788333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>1.136865</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.661654</td>\n",
       "      <td>0.602740</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.812361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.307337</td>\n",
       "      <td>0.772222</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>0.679245</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.785139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>1.350951</td>\n",
       "      <td>0.772222</td>\n",
       "      <td>0.623853</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.796944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>1.269251</td>\n",
       "      <td>0.772222</td>\n",
       "      <td>0.687023</td>\n",
       "      <td>0.633803</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.813333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.301403</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.702290</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.816944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>1.783667</td>\n",
       "      <td>0.705556</td>\n",
       "      <td>0.658065</td>\n",
       "      <td>0.536842</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.816389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1.396626</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.808333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.533229</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.661654</td>\n",
       "      <td>0.602740</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.823472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>1.666852</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.661972</td>\n",
       "      <td>0.573171</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.819444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.528962</td>\n",
       "      <td>0.761111</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.818750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.953975</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.644737</td>\n",
       "      <td>0.532609</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.814306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>1.876125</td>\n",
       "      <td>0.727778</td>\n",
       "      <td>0.662069</td>\n",
       "      <td>0.564706</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.817222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>1.612316</td>\n",
       "      <td>0.761111</td>\n",
       "      <td>0.676692</td>\n",
       "      <td>0.616438</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.817917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.597448</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.817639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.621729</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.819861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.776083</td>\n",
       "      <td>0.744444</td>\n",
       "      <td>0.684932</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.819306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.549287</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.692913</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.825694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.474955</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.807500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>1.755402</td>\n",
       "      <td>0.761111</td>\n",
       "      <td>0.671756</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.820972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.612047</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.817222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.608384</td>\n",
       "      <td>0.772222</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.650794</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.819028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.729776</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.593842</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.814861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>1.750772</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.608108</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.824583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.587069</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.667665</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.822778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.598920</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>0.689076</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.815972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.592932</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.816944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.596179</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.818889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.654597</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.819583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.607825</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>0.698413</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.818194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.597912</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>0.693548</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.817778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.606636</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>0.698413</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.817917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1350, training_loss=0.10609806701609933, metrics={'train_runtime': 294.4473, 'train_samples_per_second': 142.64, 'train_steps_per_second': 4.585, 'total_flos': 2762666081280000.0, 'train_loss': 0.10609806701609933, 'epoch': 50.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bc70f5-5957-4de9-8554-a42385ae4a5c",
   "metadata": {},
   "source": [
    "### Task 6 - display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3aecdd2966e46b7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-16T15:26:10.564304600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Results: {'eval_loss': 0.4890971779823303, 'eval_accuracy': 0.7833333333333333, 'eval_f1': 0.6355140186915887, 'eval_precision': 0.723404255319149, 'eval_recall': 0.5666666666666667, 'eval_auroc': 0.8154166666666667, 'eval_runtime': 0.449, 'eval_samples_per_second': 400.891, 'eval_steps_per_second': 13.363, 'epoch': 50.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(\"Test Set Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa7aa7-5b54-425f-8b6d-60e785ea4d60",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c021c6-1421-425b-99d1-b8ad914e8f80",
   "metadata": {},
   "source": [
    "### Task 7 - Rerank results from elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d517744-e3ef-415f-8d1d-31c3b0e3ac5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T14:54:28.204000900Z",
     "start_time": "2024-11-16T14:54:28.195483600Z"
    },
    "collapsed": false
   },
   "source": [
    "Input data into elasticsearch\n",
    "\n",
    "(This is the point where I should run elasticsearch...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ae8aaf5-a564-44ca-b7b3-f635616351d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100    21  100    21    0     0    380      0 --:--:-- --:--:-- --:--:--   381\n"
     ]
    }
   ],
   "source": [
    "!curl -X DELETE \"localhost:9200/pol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f544f5af-0113-413b-9bfd-21dce13ab8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "analyzer_settings = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"polish\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"morfologik_stem\",\n",
    "                        \"lowercase\",\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"polish\",\n",
    "                \"fields\": {\"keyword\": {\"type\": \"keyword\"}},\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b664201-72be-49e1-96ba-9d4defc0d303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'pol'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_url = \"http://localhost:9200/pol\"\n",
    "elasticsearch_headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# send the analyzer settings and mappings to elasticsearch\n",
    "response = requests.put(\n",
    "    elastic_url,\n",
    "    headers=elasticsearch_headers,\n",
    "    data=json.dumps(analyzer_settings),\n",
    ")\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7d3cf48-7695-4695-b975-bd52e004769d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "bulk_url = \"http://localhost:9200/pol/_bulk\"\n",
    "\n",
    "\n",
    "# create data index\n",
    "data = []\n",
    "for _id, text in corpus_map.items():\n",
    "    id_head = json.dumps(\n",
    "        {\"index\": {\"_index\": \"pol\", \"_id\": str(_id)}}, ensure_ascii=False\n",
    "    )\n",
    "    content = json.dumps({\"text\": text}, ensure_ascii=False)\n",
    "    data.append(id_head)\n",
    "    data.append(content)\n",
    "\n",
    "# join the bul data\n",
    "bulk_data = \"\\n\".join([item for item in data]) + \"\\n\"\n",
    "\n",
    "response = requests.post(\n",
    "    bulk_url, headers=elasticsearch_headers, data=bulk_data.encode(\"utf-8\")\n",
    ")\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json()[\"errors\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aac59d-ba17-4423-ae39-1c539a507925",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbdfab4-db2b-4e0d-9395-3aad75d86e0d",
   "metadata": {},
   "source": [
    "Compute ndcg@5 using the method from fts exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71df04e6-61e1-4a05-a3e2-02a282a14a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def ndcg_at_k(query_corpus_map, query_map, k, search_url):\n",
    "    logs = np.log2(np.arange(2, 2 + k))\n",
    "    ndcg_list = []\n",
    "    query_dict = {\"query\": {\"match\": {\"text\": {}}}, \"size\": k}\n",
    "\n",
    "    for query_id, corpus_id_list in query_corpus_map.items():\n",
    "        query_text = query_map[query_id]\n",
    "\n",
    "        query_dict[\"query\"][\"match\"][\"text\"][\"query\"] = query_text\n",
    "\n",
    "        query_request = json.dumps(query_dict)\n",
    "        response = requests.post(\n",
    "            search_url, headers=elasticsearch_headers, data=query_request\n",
    "        )\n",
    "        data = response.json()\n",
    "        hits = [int(h[\"_id\"]) for h in data[\"hits\"][\"hits\"]]\n",
    "\n",
    "        # sometimes the list of correct matches is shorter than k\n",
    "        # in  those cases we pad with 0s\n",
    "        idcg = [1 if i < len(corpus_id_list) else 0 for i in range(k)]\n",
    "        dcg = [1 if h in corpus_id_list else 0 for h in hits]\n",
    "\n",
    "        idcg = np.array(idcg) / logs\n",
    "        dcg = np.array(dcg) / logs\n",
    "\n",
    "        ndcg_list.append(dcg.sum() / idcg.sum())\n",
    "\n",
    "    return np.mean(ndcg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffede5ed-ad80-4cab-b9af-9a7ed30b5ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fiqa_qa_test = load_dataset(\"clarin-knext/fiqa-pl-qrels\")[\"test\"]\n",
    "qc_map_test = {}\n",
    "for q_id, c_id in zip(fiqa_qa_test[\"query-id\"], fiqa_qa_test[\"corpus-id\"]):\n",
    "    if q_id not in qc_map:\n",
    "        qc_map_test[q_id] = set()\n",
    "    qc_map_test[q_id].add(c_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66cb6810-da1f-444f-ad92-45e46f403e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.15957850679487598)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_at_k(qc_map_test, query_map, 10, \"http://localhost:9200/pol/_search?pretty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a6e575-cf7f-4657-9753-45f1a93f323d",
   "metadata": {},
   "source": [
    "I don't know why this score is suddenly lower than in previous lab exercises. I use the same data and I'm pretty sure the functionality is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e8f5b-4d78-4841-aab1-9a14e2dbc0d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ca0e5e-1efb-408a-a249-1fd8568693d6",
   "metadata": {},
   "source": [
    "Compute NDCG@5 with reordering using our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8269c23d-5662-46cc-88df-cd8fea9ba781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def ndcg_at_k_reordered(query_corpus_map, query_map, corpus_map, k, search_url):\n",
    "    logs = np.log2(np.arange(2, 2 + k))\n",
    "    ndcg_list = []\n",
    "    query_dict = {\"query\": {\"match\": {\"text\": {}}}, \"size\": 4 * k}\n",
    "\n",
    "    for query_id, corpus_id_list in tqdm(\n",
    "        query_corpus_map.items(), total=len(query_corpus_map)\n",
    "    ):\n",
    "        query_text = query_map[query_id]\n",
    "\n",
    "        query_dict[\"query\"][\"match\"][\"text\"][\"query\"] = query_text\n",
    "\n",
    "        query_request = json.dumps(query_dict)\n",
    "        response = requests.post(\n",
    "            search_url, headers=elasticsearch_headers, data=query_request\n",
    "        )\n",
    "        data = response.json()\n",
    "        hits = [int(h[\"_id\"]) for h in data[\"hits\"][\"hits\"]]\n",
    "\n",
    "        # this loop is the key. We order the data by probability of belonging to the positive class\n",
    "        data_to_reorder = []\n",
    "        for h in hits:\n",
    "            inp = query_text + f\" [SEP] \" + corpus_map[h]\n",
    "            inp = tokenizer(\n",
    "                inp, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "            ).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inp)\n",
    "            score = outputs.logits[0, 1].item()\n",
    "            data_to_reorder.append((h, score))\n",
    "        reordered_data = sorted(data_to_reorder, key=lambda x: x[1], reverse=True)[:k]\n",
    "        hits = [d[0] for d in reordered_data]\n",
    "\n",
    "        # sometimes the list of correct matches is shorter than k\n",
    "        # in  those cases we pad with 0s\n",
    "        idcg = [1 if i < len(corpus_id_list) else 0 for i in range(k)]\n",
    "        dcg = [1 if h in corpus_id_list else 0 for h in hits]\n",
    "\n",
    "        idcg = np.array(idcg) / logs\n",
    "        dcg = np.array(dcg) / logs\n",
    "\n",
    "        ndcg_list.append(dcg.sum() / idcg.sum())\n",
    "\n",
    "    return np.mean(ndcg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb898bb9-13aa-429f-8914-f0fd2a6363d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 648/648 [05:16<00:00,  2.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.049053581569495804)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_at_k_reordered(\n",
    "    qc_map_test, query_map, corpus_map, 10, \"http://localhost:9200/pol/_search?pretty\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee94196-beb4-4f3b-80c6-306fb7918b26",
   "metadata": {},
   "source": [
    "We see a very low score. It's hardly a surprise since the model has been trained on a small fraction of the full dataset. There are so many different queries and potential answers that the reordering given by our model looks practically random. \n",
    "\n",
    "It's also possible that the order of the data in the query dataset groups it by categories. If that's the case then my model was probably trained on answers from the first category (probably something regarding finances) as the \"correct answers\" and random answers from all other categories as \"negative answers\". I don't know. But debugging the dataset is not the point of the laboratory. I learned how to train huggingface models for classification tasks in nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1410cc5-6a7f-4b32-9547-903921d38b78",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b78f5-02ab-442f-a0f0-2921fd6dcb74",
   "metadata": {},
   "source": [
    "\n",
    "- Do you think simpler methods, like Bayesian bag-of-words model, would work for sentence-pair classification? Justify your answer.\n",
    "    - Bayesian models don't have the understanding of language and work purely on the word occurences. Classifying sentence pairs like we did in this exercise could be problematic. The model doesn't account for context and can't tell which words come before and which come after the separator token. If we wanted to do that we'd have to encode a word from before the separater differently than the word coming after the separator.\n",
    "- What hyper-parameters you have selected for the training? What resources (papers, tutorial) you have consulted to select these hyper-parameters?\n",
    "    - No studies. Just trial and error. I found out that in this specific case lowering the batch-size and decreasing the learning rate helped me get the best results.\n",
    "- Think about pros and cons of the neural-network models with respect to natural language processing. Provide at least 2 pros and 2 cons.\n",
    "    - Pros:\n",
    "        - Model's understanding of language. Possibility of better classification(if we manage to train the model properly).\n",
    "        - Great flexibility. It's easy to taylor a deep learning model to a specific task.\n",
    "    - Cons: \n",
    "        - Heavy training and inference - Scoring multiple data elements is much slower than with FTS. \n",
    "        - Need for a large amounts of training data. Bad for small tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af0264e-65c4-4252-8e43-184506221a72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
