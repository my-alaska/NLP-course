{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3657b63896964595",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# NER vs LLM\n",
    "\n",
    "---\n",
    "\n",
    "### Configure OLLama\n",
    "\n",
    "Install OLLama\n",
    "\n",
    "Open terminal and type:\n",
    "\n",
    "`ollama run SpeakLeash/bielik-11b-v2.3-instruct:Q4_K_M`\n",
    "\n",
    "Then set up chat with ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4d3081191a8c23",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:37:47.232217100Z",
     "start_time": "2024-12-08T17:37:28.065149800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Named Entity Recognition (NER) to zadanie przetwarzania języka naturalnego, które polega na identyfikacji i klasyfikacji nazwanych jednostek w tekście. Nazwane jednostki to takie elementy jak osoby, organizacje, lokalizacje, daty, ilości itp., które mają specjalne znaczenie dla kontekstu i mogą być użyteczne do dalszej analizy tekstu.\\n\\nProblem NER polega na tym, że teksty w języku naturalnym są często niejednoznaczne i zawierają wiele słów, które mogą mieć różne znaczenia w zależności od kontekstu. Na przykład, słowo \"John\" może być imieniem osoby lub nazwą miejsca. Zadaniem NER jest rozpoznanie tych jednostek i przypisanie im odpowiednich etykiet (np. osoba, organizacja, lokalizacja itp.).\\n\\nProblem ten jest trudny z kilku powodów:\\n\\n1. **Kontekst**: Słowa mogą mieć różne znaczenia w zależności od kontekstu, w którym są używane. Na przykład, słowo \"bank\" może oznaczać instytucję finansową lub brzeg rzeki.\\n2. **Niejednoznaczność**: Niektóre słowa mogą być nazwanymi jednostkami w pewnych kontekstach, ale nie w innych. Na przykład, \"Apple\" może oznaczać firmę technologiczną lub owoc.\\n3. **Rozpoznawanie granic**: NER musi również rozpoznać granice nazwanych jednostek, co jest trudne, ponieważ niektóre jednostki mogą być długie i składać się z wielu słów (np. \"Apple Inc.\").\\n4. **Różnorodność językowa**: Różne języki mają różne konwencje dotyczące nazywania jednostek, co może utrudniać rozpoznawanie tych samych jednostek w różnych językach.\\n5. **Zmienność**: Nazwy mogą się zmieniać z czasem (np. \"Google\" zostało przemianowane na \"Alphabet Inc.\"), co może wprowadzać dodatkową trudność w utrzymaniu aktualności modeli NER.\\n\\nRozwiązanie tych problemów wymaga zaawansowanych technik uczenia maszynowego, takich jak sieci neuronowe rekurencyjne (RNN), konwolucyjne sieci neuronowe (CNN) oraz modele oparte na transformerach (np. BERT, GPT). Te modele są trenowane na dużych zbiorach danych zawierających oznaczone nazwane jednostki i mogą być dostosowywane do specyficznych domen lub języków.\\n\\nPodsumowując, problem Named Entity Recognition jest kluczowym wyzwaniem w przetwarzaniu języka naturalnego, które ma wiele zastosowań, takich jak ekstrakcja informacji, analiza sentymentu, systemy rekomendacyjne i wiele innych. Rozwiązanie tego problemu wymaga zaawansowanych technik uczenia maszynowego oraz dużych zbiorów danych do trenowania modeli.'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "def chat_ollama(text):\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"SpeakLeash/bielik-11b-v2.3-instruct:Q4_K_M\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": text}],\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload)\n",
    "\n",
    "    # Parse the response\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"message\"][\"content\"]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "question = \"Możesz opowiedzieć mi o problemie Named Entity Recognition?\"\n",
    "\n",
    "chat_ollama(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using `generate` endpoint"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2594ec7d3bc4375c"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53109ee9844a45c5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:37:47.233223500Z",
     "start_time": "2024-12-08T17:37:47.231043600Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "\n",
    "def prompt_ollama(text):\n",
    "    payload = {\n",
    "        \"model\": \"SpeakLeash/bielik-11b-v2.3-instruct:Q4_K_M\",\n",
    "        \"prompt\": text,\n",
    "        \"context\": [],\n",
    "        \"options\": {\"top_k\": 10, \"temperature\": 0},\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, data=json.dumps(payload))\n",
    "\n",
    "    # Parse the response\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"response\"]\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'Tak, potrafię mówić po polsku. W czym mogę Ci pomóc?'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_ollama(\"Czy potrafisz mówić po polsku?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:37:52.279547500Z",
     "start_time": "2024-12-08T17:37:49.730830100Z"
    }
   },
   "id": "9eb6ff8633ef6c8e"
  },
  {
   "cell_type": "markdown",
   "id": "d4d3a6061ed99ee4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Take 1000 passages from fiqa corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8659417aeb1d6159",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:37:55.433076Z",
     "start_time": "2024-12-08T17:37:52.270549800Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from numpy.random import choice\n",
    "\n",
    "fiqa_corpus = load_dataset(\"clarin-knext/fiqa-pl\", \"corpus\")[\"corpus\"]\n",
    "\n",
    "fiqa_corpus = fiqa_corpus[\"text\"]\n",
    "fiqa_idx = choice(len(fiqa_corpus), 100, replace=False)\n",
    "fiqa_corpus = [fiqa_corpus[i] for i in fiqa_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11459d8c3dacc20f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "### NER baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb79bacc2f8db69c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:37:59.616408900Z",
     "start_time": "2024-12-08T17:37:55.434076500Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pl_core_news_sm\")\n",
    "\n",
    "\n",
    "def get_ents(text):\n",
    "    doc = nlp(text)\n",
    "    entity_dict = {}\n",
    "    for ent in doc.ents:\n",
    "        text, label = ent.text, ent.label_\n",
    "        if (text, label) not in entity_dict.keys():\n",
    "            entity_dict[(text, label)] = 0\n",
    "        entity_dict[(text, label)] += 1\n",
    "    return entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abb224b7fb0f5fe4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:37:59.629622700Z",
     "start_time": "2024-12-08T17:37:59.618713700Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"Chcę, abyś wymienił encje nazwane w polskim tekście, który Ci podam, i przypisał je do jednej z 6 kategorii.\"\n",
    "    \"Encje nazwane (eng. named entities) określają wybraną charakterystyczną jednostkę za pomocą zdefiniowanej nazwy.\"\n",
    "    \"6 kategorii, do których powinieneś przypisać encje nazwane: \\n\"\n",
    "    \"date - format daty/czasu: precyzyjna data, rok, itp. np. 11 Września. \\n\"\n",
    "    \"geogName - charakterystyczne obiekty geograficzne: rzeki, góry, itp. np. Wisła, Giewont. \\n\"\n",
    "    \"orgName - pełne nazwy organizacji: nazwy firm, itp. np. Microsoft, Bank PKO. \"\n",
    "    \"persName - znane osoby: politycy, celebryci, postacie historyczne itp. np. Artur Rojek, Papież Franciszek. \\n\"\n",
    "    \"placeName - jednostki polityczne: miasta, państwa, itd. np. Kraków, Czechy, Australia, Bojszowy Nowe. \\n\"\n",
    "    \"time - Godzina. np. 18:30. \\n\"\n",
    "    \"Zignoruj fragmenty tekstu nie będące charakterystycznymi encjami nazwanymi. \"\n",
    "    \"Wynik powinien być listą krotek. Każda krotka powinna składać się z encji nazwanej i odpowiadającej jej kategorii (jednej z wymienionych). \"\n",
    "    \"Przykładowy format poprawnego wyniku dla tekstu: \\n\"\n",
    "    '\"Ostatnim razem w górach izerskich byłem 11 września\": [(\"góry izerskie\", \"geogName\"), (\"11 września\", \"date\")] \\n'\n",
    "    '\"Nigdy nie byłem w stanach zjednoczonych, ale wiem kto to Barack Obama\": [(\"Stany Zjednoczone\", \"placeName\"), (\"Barack Obama\", \"persName\")] \\n'\n",
    "    \"Nie dodawaj żadnych dodatkowych informacji, wyjaśnień, ani kodu w swojej odpowiedzi. Zwróć jedynie poprawną listą języka Python. \"\n",
    "    \"Tekst: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_list(text):\n",
    "    regex = r\"\\[[^\\[\\]]*\\]\"\n",
    "    return re.findall(regex, text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:37:59.630623100Z",
     "start_time": "2024-12-08T17:37:59.620720100Z"
    }
   },
   "id": "48494c59a77123ba"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb30282b-1ff5-441d-b6b5-ec7cb5041d9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T17:37:59.630623100Z",
     "start_time": "2024-12-08T17:37:59.623231200Z"
    }
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "# We use spacy to lemmatize if possible as the llm seems to be oblivious to the intricacy of case declension\n",
    "def get_llm_ents(text):\n",
    "    response = chat_ollama(prompt + text)\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    lists = extract_list(response)\n",
    "    if not lists or len(lists[0]) == 0:\n",
    "        return {}\n",
    "    response = lists[0]\n",
    "\n",
    "    entity_dict = {}\n",
    "    response = literal_eval(response)\n",
    "    if len(response) == 0 or len(response[0]) != 2:\n",
    "        return {}\n",
    "    for entity, label in response:\n",
    "        ents_tmp = list(nlp(entity).sents)\n",
    "        if len(ents_tmp):\n",
    "            lemma = \" \".join([l.lemma_ for l in ents_tmp[0]])\n",
    "        else:\n",
    "            lemma = entity\n",
    "        if (lemma, label) not in entity_dict.keys():\n",
    "            entity_dict[(lemma, label)] = 0\n",
    "        entity_dict[(lemma, label)] += 1\n",
    "    return entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d7801ef-7f2d-439b-b00b-c9f54cfaf1f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T17:38:02.832045700Z",
     "start_time": "2024-12-08T17:37:59.627623100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"Kraków\", \"placeName\"), (\"Polska\", \"placeName\"), (\"Stany Zjednoczone\", \"placeName\"), (\"Michael Jackson\", \"persName\")]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{('Kraków', 'placeName'): 1,\n ('Polska', 'placeName'): 1,\n ('Stany Zjednoczone', 'placeName'): 1,\n ('Michael Jackson', 'persName'): 1}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = \"Wczoraj w Krakowie miało miejsce spotkanie prezydentów Polski i Stanów Zjednoczonych. Polacy cieszyli się z wizyty Michaela Jacksona\"\n",
    "\n",
    "get_llm_ents(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e7325-b82d-4a08-a2ec-1d98496830db",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0943b14-2001-4aca-b3b4-467504704cd2",
   "metadata": {},
   "source": [
    "### Compare spaCy to Bielik\n",
    "\n",
    "I gave up on trying to run it without providing the examples.\n",
    "\n",
    "The returned results were so bad that parsing the outputs was practically impossible.\n",
    "\n",
    "It's safe to conclude that few shot learning can drastically improve the quality of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "save_path = Path(\"data\")\n",
    "save_path.mkdir(exist_ok=True, parents=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:38:02.851694800Z",
     "start_time": "2024-12-08T17:38:02.832045700Z"
    }
   },
   "id": "71371e038e294c8c"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dabaf2a-a8f5-4bb5-961b-16ba06500e2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T17:38:02.852696600Z",
     "start_time": "2024-12-08T17:38:02.835969200Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "if not (save_path / \"fiqa_spacy.pkl\").exists():\n",
    "    fiqa_spacy = [get_ents(text) for text in tqdm(fiqa_corpus)]\n",
    "\n",
    "    with open(save_path / \"fiqa_spacy.pkl\", \"wb\") as fp:\n",
    "        pickle.dump(fiqa_spacy, fp)\n",
    "\n",
    "with open(save_path / \"fiqa_spacy.pkl\", \"rb\") as fp:\n",
    "    fiqa_spacy = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f432afc-2caa-441b-841c-b7790c8585cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T17:38:02.885047600Z",
     "start_time": "2024-12-08T17:38:02.849695800Z"
    }
   },
   "outputs": [],
   "source": [
    "if not (save_path / \"fiqa_llm.pkl\").exists():\n",
    "    fiqa_llm = []\n",
    "    for text in tqdm(fiqa_corpus):\n",
    "        fiqa_llm.append(get_llm_ents(text))\n",
    "\n",
    "    with open(save_path / \"fiqa_llm.pkl\", \"wb\") as fp:\n",
    "        pickle.dump(fiqa_llm, fp)\n",
    "\n",
    "with open(save_path / \"fiqa_llm.pkl\", \"rb\") as fp:\n",
    "    fiqa_llm = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7ea8071-179c-41ba-9fe1-53529053eba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T17:38:02.886047900Z",
     "start_time": "2024-12-08T17:38:02.862609900Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_ent_cat_count(entity_dict_list):\n",
    "    entities, categories = {}, {}\n",
    "    for d in entity_dict_list:\n",
    "        for (entity, category), count in d.items():\n",
    "            if entity not in entities.keys():\n",
    "                entities[entity] = 0\n",
    "            if category not in categories.keys():\n",
    "                categories[category] = 0\n",
    "            entities[entity] += count\n",
    "            categories[category] += count\n",
    "    entities = sorted(list(entities.items()), key=lambda x: x[1], reverse=True)\n",
    "    categories = sorted(list(categories.items()), key=lambda x: x[1], reverse=True)\n",
    "    return entities, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c4dcbaa-76d3-449a-ba0a-62b11c23c820",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T17:38:02.886047900Z",
     "start_time": "2024-12-08T17:38:02.865181100Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy_entities, spacy_categories = get_ent_cat_count(fiqa_spacy)\n",
    "llm_entities, llm_categories = get_ent_cat_count(fiqa_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "[('orgName', 141),\n ('placeName', 70),\n ('persName', 66),\n ('date', 18),\n ('geogName', 16)]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_categories"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:38:02.886047900Z",
     "start_time": "2024-12-08T17:38:02.868333800Z"
    }
   },
   "id": "4a5904d6052a5d32"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[('CFA', 23),\n ('MBA', 16),\n ('USA', 12),\n ('Google', 5),\n ('HSA', 5),\n ('Indiach', 4),\n ('SS', 4),\n ('LLC', 4),\n ('eBay', 3),\n ('Forex', 3),\n ('IRS', 3),\n ('SIP', 3),\n ('Amazon', 3),\n ('CFD', 3),\n ('Apple', 3),\n ('Sydney', 3),\n ('CBV', 3),\n ('Stanach Zjednoczonych', 2),\n ('TPS', 2),\n ('amerykańskimi', 2),\n ('Wielkiej Brytanii', 2),\n ('BB', 2),\n ('Columbia', 2),\n ('Whartona', 2),\n ('Aptery', 2)]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_entities[:25]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:38:02.886047900Z",
     "start_time": "2024-12-08T17:38:02.872999600Z"
    }
   },
   "id": "fba58294e86ed16"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a82c125-5a74-4161-882a-45403312aa74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T17:38:02.886047900Z",
     "start_time": "2024-12-08T17:38:02.877130800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('orgName', 123),\n ('date', 55),\n ('placeName', 32),\n ('time', 17),\n ('persName', 15),\n ('geogName', 6),\n ('service', 4),\n ('currency', 2),\n ('', 2),\n ('object', 2),\n ('number', 1)]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6af82cb6-61e7-4d29-97e8-93a121e9142c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T17:38:02.887054800Z",
     "start_time": "2024-12-08T17:38:02.881789700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('1987', 33),\n ('Stany Zjednoczone', 7),\n ('wielki Brytania', 3),\n ('Barack Obama', 2),\n ('Amazon', 2),\n ('USA', 2),\n ('rezerwa Federalny', 2),\n ('UPS', 2),\n ('USPS', 2),\n ('fałszywy równoważność', 1),\n ('być prawie pewny', 1),\n ('dane', 1),\n ('lewicow źródło', 1),\n ('przeciwnik', 1),\n ('naukowy', 1),\n ('Mindwin', 1),\n ('rynek rolny', 1),\n ('rynek towarowy', 1),\n ('uniwersytet', 1),\n ('kawiarnia', 1),\n ('bank godzina', 1),\n ('Fort Lauderdale', 1),\n ('elektrownia na paliwo kopalny', 1),\n ('Wall Street', 1),\n ('Brexit', 1)]"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_entities[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "343ee9b0254f5aeb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the results of llm retrieval is horrible in comparison to spaCy's ner. The model hallucinated several new classes and has numerous false positives."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "245acdcc3fb56706"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f58cc1c7-93ec-4d3e-8fa1-93488f4d9b6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T17:56:42.872657Z",
     "start_time": "2024-12-08T17:56:42.840372100Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"annotations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "class_map = {\n",
    "    \"PER\": \"persName\",\n",
    "    \"LOC\": \"placeName\",\n",
    "    \"ORG\": \"orgName\",\n",
    "    \"GPE\": \"geogName\",\n",
    "    \"DATE\": \"date\",\n",
    "    \"TIME\": \"time\",\n",
    "}\n",
    "\n",
    "\n",
    "def extract_from_annotation(annotation):\n",
    "    entity_dict = {}\n",
    "    for entity, category, _ in annotation:\n",
    "        ents_tmp = list(nlp(entity).sents)\n",
    "        if len(ents_tmp):\n",
    "            entity = \" \".join([l.lemma_ for l in ents_tmp[0]])\n",
    "\n",
    "        category = class_map[category]\n",
    "        if (entity, category) not in entity_dict.keys():\n",
    "            entity_dict[(entity, category)] = 0\n",
    "        entity_dict[(entity, category)] += 1\n",
    "    return entity_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:56:43.134664Z",
     "start_time": "2024-12-08T17:56:43.129423200Z"
    }
   },
   "id": "843bd5c58895f247"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "texts = df[\"Tekst\"]\n",
    "entities = [extract_from_annotation(literal_eval(e)) for e in df[\"Encje\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:56:43.537693400Z",
     "start_time": "2024-12-08T17:56:43.338598300Z"
    }
   },
   "id": "134b9f579190aa1f"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "{('bokserski', 'placeName'): 1,\n ('AT&T Stadium', 'geogName'): 1,\n ('Arlington', 'placeName'): 1,\n ('Teksasie Jake Paul', 'placeName'): 1,\n (\"Mike'a Tysona\", 'persName'): 1,\n ('Sylvester Stallone', 'persName'): 1}"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ents(texts[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:56:43.657292300Z",
     "start_time": "2024-12-08T17:56:43.625501Z"
    }
   },
   "id": "1a81c0f54c5ae3ca"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "if not (save_path / \"predictions.pkl\").exists():\n",
    "    predictions = []\n",
    "\n",
    "    for text in tqdm(texts):\n",
    "        predictions.append(get_llm_ents(text))\n",
    "\n",
    "    with open(save_path / \"predictions.pkl\", \"wb\") as fp:\n",
    "        pickle.dump(predictions, fp)\n",
    "\n",
    "with open(save_path / \"predictions.pkl\", \"rb\") as fp:\n",
    "    predictions = pickle.load(fp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:56:43.868102300Z",
     "start_time": "2024-12-08T17:56:43.865931400Z"
    }
   },
   "id": "60354d9207c42124"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def get_TP_FP_FN(y_test, y_pred, use_class=False):\n",
    "    if not use_class:\n",
    "        y_test = {k[0]: v for k, v in y_test.items()}\n",
    "        y_pred = {k[0]: v for k, v in y_pred.items()}\n",
    "\n",
    "    TP = FP = FN = 0\n",
    "\n",
    "    for e in y_pred:\n",
    "        if e in y_test:\n",
    "            pred = y_pred[e]\n",
    "            test = y_test[e]\n",
    "            if pred == test:\n",
    "                TP += pred\n",
    "            elif pred > test:\n",
    "                TP += test\n",
    "                FP += pred - test\n",
    "            else:\n",
    "                TP += pred\n",
    "                FN += test - pred\n",
    "        else:\n",
    "            FP += y_pred[e]\n",
    "\n",
    "    for e in y_test:\n",
    "        if e not in y_pred:\n",
    "            FN += y_test[e]\n",
    "\n",
    "    return TP, FP, FN"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:56:44.150409Z",
     "start_time": "2024-12-08T17:56:44.147993200Z"
    }
   },
   "id": "937d1dd0c12f1e7d"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "scores = [get_TP_FP_FN(t, p) for t, p in zip(entities, predictions)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:56:44.464835Z",
     "start_time": "2024-12-08T17:56:44.459321200Z"
    }
   },
   "id": "7bcbbc79c18a407c"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "TP = sum([scr[0] for scr in scores])\n",
    "FP = sum([scr[1] for scr in scores])\n",
    "FN = sum([scr[2] for scr in scores])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:56:44.764769400Z",
     "start_time": "2024-12-08T17:56:44.759532900Z"
    }
   },
   "id": "7105c1c583a44b32"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "(58, 11, 36)"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP, FP, FN"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:56:45.018912500Z",
     "start_time": "2024-12-08T17:56:45.014917800Z"
    }
   },
   "id": "a0f41c717b218e9e"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "prec = TP / (TP + FP)\n",
    "rec = TP / (TP + FN)\n",
    "f1 = 2 * (prec * rec) / (prec + rec)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:56:45.348757400Z",
     "start_time": "2024-12-08T17:56:45.343245100Z"
    }
   },
   "id": "163edae8b1661970"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission : 0.8405797101449275\n",
      "Recall     : 0.6170212765957447\n",
      "f1 score   : 0.7116564417177915\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precission : {prec}\")\n",
    "print(f\"Recall     : {rec}\")\n",
    "print(f\"f1 score   : {f1}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:56:45.642811300Z",
     "start_time": "2024-12-08T17:56:45.634297800Z"
    }
   },
   "id": "c919bc57e2b9d83d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model skips around 40% of entities it should be detecting but it only 15% of it's predictions are false. \n",
    "\n",
    "I'm surprised by how well it worked"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d01ca1fface3a68d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check the same scores for spaCy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13d1ddd69d38c70c"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# this function converts found annotation to normalized form\n",
    "# (same one as the grand truth - extract_from_annotation function above)\n",
    "# Some words are detected and not declined\n",
    "# We want to make sure that the same entity is returned in the same way\n",
    "\n",
    "\n",
    "def normalize_entity_dict(annotation):\n",
    "    entity_dict = {}\n",
    "    for (entity, category), _ in annotation.items():\n",
    "        ents_tmp = list(nlp(entity).sents)\n",
    "        if len(ents_tmp):\n",
    "            entity = \" \".join([l.lemma_ for l in ents_tmp[0]])\n",
    "\n",
    "        if (entity, category) not in entity_dict.keys():\n",
    "            entity_dict[(entity, category)] = 0\n",
    "        entity_dict[(entity, category)] += 1\n",
    "    return entity_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:56:47.024749200Z",
     "start_time": "2024-12-08T17:56:47.022314100Z"
    }
   },
   "id": "b8322a4795c7205e"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "predictions_spacy = [normalize_entity_dict(get_ents(t)) for t in texts]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:56:47.897148Z",
     "start_time": "2024-12-08T17:56:47.596863600Z"
    }
   },
   "id": "4c40ce0ccf344038"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "(54, 34, 40)"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [get_TP_FP_FN(t, p) for t, p in zip(entities, predictions_spacy)]\n",
    "\n",
    "TP = sum([scr[0] for scr in scores])\n",
    "FP = sum([scr[1] for scr in scores])\n",
    "FN = sum([scr[2] for scr in scores])\n",
    "\n",
    "TP, FP, FN"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:56:48.540386600Z",
     "start_time": "2024-12-08T17:56:48.538344400Z"
    }
   },
   "id": "7588e34e002943a4"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission : 0.6136363636363636\n",
      "Recall     : 0.574468085106383\n",
      "f1 score   : 0.5934065934065934\n"
     ]
    }
   ],
   "source": [
    "prec = TP / (TP + FP)\n",
    "rec = TP / (TP + FN)\n",
    "f1 = 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "print(f\"Precission : {prec}\")\n",
    "print(f\"Recall     : {rec}\")\n",
    "print(f\"f1 score   : {f1}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:56:49.668411700Z",
     "start_time": "2024-12-08T17:56:49.635012300Z"
    }
   },
   "id": "154ab658d9494d00"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see the scores are worse! Let us compare on an example"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92970b4a72bd3e4c"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Niemal dwa tygodnie temu doszło do walki, na którą czekał cały bokserski świat. Na AT&T Stadium w Arlington w Teksasie Jake Paul pokonał Mike'a Tysona po jednogłośnej decyzji sędziów. Część kibiców i ekspertów twierdzi jednak, że pojedynek... mógł być ustawiony. Do tego grona najwyraźniej należy Sylvester Stallone, który po ogłoszeniu werdyktu wygłosił dosyć kontrowersyjną opinię. Teraz popularny aktor postanowił przeprosić za swoje słowa.\n",
      "\n",
      "{('jake Paul', 'persName'): 1, ('Mike Tyson', 'persName'): 1, ('sylvester Stallone', 'persName'): 1, ('aT&T stadium', 'orgName'): 1, ('Arlington', 'geogName'): 1, ('Teksas', 'geogName'): 1}\n",
      "\n",
      "{('jake Paul', 'persName'): 1, ('Mike Tyson', 'persName'): 1, ('aT&T stadium', 'orgName'): 1, ('Arlington', 'placeName'): 1, ('Teksas', 'placeName'): 1}\n",
      "\n",
      "{('bokserski', 'placeName'): 1, ('aT&T stadium', 'geogName'): 1, ('Arlington', 'placeName'): 1, ('Teksasie jake Paul', 'placeName'): 1, ('Mike Tysona', 'persName'): 1, ('sylvester Stallone', 'persName'): 1}\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])\n",
    "print()\n",
    "print(entities[0])\n",
    "print()\n",
    "print(predictions[0])\n",
    "print()\n",
    "print(predictions_spacy[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-08T17:56:51.408972100Z",
     "start_time": "2024-12-08T17:56:51.402267200Z"
    }
   },
   "id": "9f368b6c3725f576"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the LLM proved to work better than spacy! It's a surprising result. Seeing the performance of LLM on fiqa dataset we could see that it detected a lot of incorrect entities.\n",
    "\n",
    "It's possible that hand annotated grand-truth sentences are easier for the model to work with. They are also shorter, and that makes the context much simpler.\n",
    "\n",
    "In conclusion. I'd lean towards using spacy when I need to process a lot of data or long texts. It's much faster. If I want precise answers for simpler or shorter texts I'd consider using an LLM instead.\n",
    "\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. How does the performance of LLM-based NER compare to traditional approaches? What are the trade-offs in terms of accuracy, speed, and resource usage?\n",
    "    - The LLM-based NER works better on simple data. However, it's much slower and might not be great for larger tasks\n",
    "2. Which prompting strategy proved most effective for NER and classification tasks? Why?\n",
    "    - LLM-based NER needed examples to work properly and return even remotely acceptable output. Without it even the format of the output string was incorrect.\n",
    "3. What are the limitations and potential biases of using LLMs for NER and classification?\n",
    "    - LLM are very slow and inaccurate for longer texts. Long text would need a more accurate and probably larger model. It can be expensive and resource-consuming.\n",
    "4. In what scenarios would you recommend using traditional NER vs. LLM-based approaches?\n",
    "    - Described above"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a267c80676c8066"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d6cf4286dba2e203"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
