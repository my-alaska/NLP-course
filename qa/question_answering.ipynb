{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef6e1e2e0a6a2be",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load simple-legal-questions-pl dataset\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f0d26d5f9c51f3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T20:50:25.513573300Z",
     "start_time": "2025-01-08T20:50:24.553915Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "passage_dict = {}\n",
    "title_dict = {}\n",
    "with open(\"data/passages.jl\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        d = json.loads(line)\n",
    "        passage_dict[d[\"_id\"]] = d[\"text\"]\n",
    "        title_dict[d[\"_id\"]] = d[\"title\"]\n",
    "\n",
    "question_dict = {}\n",
    "with open(\"data/questions.jl\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        d = json.loads(line)\n",
    "        question_dict[d[\"_id\"]] = d[\"text\"]\n",
    "\n",
    "answer_dict = {}\n",
    "with open(\"data/answers.jl\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        d = json.loads(line)\n",
    "        answer_dict[d[\"question-id\"]] = d[\"answer\"]\n",
    "\n",
    "test_dataset = {\n",
    "    \"id\": [],\n",
    "    \"title\": [],\n",
    "    \"context\": [],\n",
    "    \"question\": [],\n",
    "    \"answers\": [],\n",
    "}\n",
    "\n",
    "with open(\"data/relevant.jl\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        d = json.loads(line)\n",
    "        # test only on questions that have answers\n",
    "        if d[\"question-id\"] not in answer_dict or answer_dict[d[\"question-id\"]] == \"\":\n",
    "            continue\n",
    "        test_dataset[\"id\"].append(d[\"question-id\"])\n",
    "        test_dataset[\"title\"].append(title_dict[d[\"passage-id\"]])\n",
    "        test_dataset[\"context\"].append(passage_dict[d[\"passage-id\"]])\n",
    "        test_dataset[\"question\"].append(question_dict[d[\"question-id\"]])\n",
    "        test_dataset[\"answers\"].append(answer_dict[d[\"question-id\"]])\n",
    "\n",
    "test_dataset = Dataset.from_dict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31662f3b30b2da1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load and process PoQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T20:50:28.024094700Z",
     "start_time": "2025-01-08T20:50:25.513864Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "poquad = load_dataset(\"clarin-pl/poquad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da83fdf5a9b0b77f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T20:50:28.030942700Z",
     "start_time": "2025-01-08T20:50:28.026432700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load(dataset, path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    answers = []\n",
    "    for data_dict in data[\"data\"]:\n",
    "        for p in data_dict[\"paragraphs\"]:\n",
    "            for qa in p[\"qas\"]:\n",
    "                if qa[\"is_impossible\"]:\n",
    "                    continue\n",
    "                for answer in qa[\"answers\"]:\n",
    "                    answers.append(answer[\"generative_answer\"])\n",
    "\n",
    "    poquad_dataset = {\n",
    "        \"id\": dataset[\"id\"],\n",
    "        \"title\": dataset[\"title\"],\n",
    "        \"context\": dataset[\"context\"],\n",
    "        \"question\": dataset[\"question\"],\n",
    "        \"answers\": answers,\n",
    "    }\n",
    "\n",
    "    return poquad_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b58ed4caab06ed2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T20:50:28.936808700Z",
     "start_time": "2025-01-08T20:50:28.028942700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = load(poquad[\"train\"], \"poquad-train.json\")\n",
    "train_dataset = Dataset.from_dict(train_dataset)\n",
    "\n",
    "valid_dataset = load(poquad[\"validation\"], \"poquad-dev.json\")\n",
    "valid_dataset = Dataset.from_dict(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f37e68b5df86d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load and prepare plt5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "992ef66a9d3e028a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T20:50:30.299109600Z",
     "start_time": "2025-01-08T20:50:28.937809200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a0522bd79a8435",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-08T20:50:30.295109100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/plt5-base\")\n",
    "\n",
    "# # Useful class\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"allegro/plt5-base\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"results/checkpoint-8661\")\n",
    "model.to(device)\n",
    "[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85dd291a6d144b83",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-08T20:50:30.296109500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37812fd0adba44328dd7f316cbade936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46187 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piotr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e593defdf24635a602a13ddc5c995f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5764 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6a58ad21914275a9cb8946fb8db6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/593 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(examples):\n",
    "    inputs = [\n",
    "        f\"pytanie: {q} \\n kontekst: {c}\"\n",
    "        for q, c in zip(examples[\"question\"], examples[\"context\"])\n",
    "    ]\n",
    "    targets = examples[\"answers\"]\n",
    "\n",
    "    # Tokenizing inputs and targets\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Tokenizing targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=128,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "    # Replace padding token id for labels to ignore index (-100) in loss computation\n",
    "    labels[\"input_ids\"] = [\n",
    "        [l if l != tokenizer.pad_token_id else -100 for l in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Create dataset and preprocess\n",
    "\n",
    "train_dataset_preprocessed = train_dataset.map(preprocess, batched=True)\n",
    "valid_dataset_preprocessed = valid_dataset.map(preprocess, batched=True)\n",
    "test_dataset_preprocessed = test_dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d25ad10b9731338",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-08T20:50:30.298109300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\piotr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=2000,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=2000,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_preprocessed,\n",
    "    eval_dataset=valid_dataset_preprocessed,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# # Training done. Now we load the model from checkpoint\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c947ee-ddc1-47c3-b3a0-1110f89f6ec5",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2cb85e9c08e2c44f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-08T20:50:30.298109300Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "exact_match = load(\"exact_match\")\n",
    "\n",
    "\n",
    "def compute_f1(predictions, labels):\n",
    "    f1 = 0\n",
    "    for predicted_tokens, true_tokens in zip(predictions, labels):\n",
    "        true_tokens = set(true_tokens.tolist())\n",
    "        predicted_tokens = set(predicted_tokens.tolist())\n",
    "\n",
    "        if 0 in true_tokens:\n",
    "            true_tokens.remove(0)\n",
    "        if 0 in predicted_tokens:\n",
    "            predicted_tokens.remove(0)\n",
    "\n",
    "        # Compute the intersection\n",
    "        common_tokens = true_tokens.intersection(predicted_tokens)\n",
    "\n",
    "        # Compute precision and recall\n",
    "        precision = (\n",
    "            len(common_tokens) / len(predicted_tokens) if predicted_tokens else 0\n",
    "        )\n",
    "        recall = len(common_tokens) / len(true_tokens) if true_tokens else 0\n",
    "\n",
    "        # Compute F1 score\n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        f1 += 2 * precision * recall / (precision + recall)\n",
    "    f1 = f1 / len(predictions)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def compute_metrics(predictions, labels):\n",
    "\n",
    "    encoded_preds = tokenizer(\n",
    "        predictions,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    encoded_labels = tokenizer(\n",
    "        labels,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    print(\"computing metrics\")\n",
    "    em = exact_match.compute(predictions=predictions, references=labels)[\"exact_match\"]\n",
    "    f1 = compute_f1(encoded_preds, encoded_labels)\n",
    "\n",
    "    return em, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bffb1d-ce35-4849-bd20-ded888952d17",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61359116-ed2e-4829-82cd-191d08456b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='399' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 06:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.360666275024414, 'eval_model_preparation_time': 0.003, 'eval_runtime': 8.9475, 'eval_samples_per_second': 66.276, 'eval_steps_per_second': 4.247}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate(eval_dataset=test_dataset_preprocessed)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506a590-04c2-49d9-b811-61632f82aa00",
   "metadata": {},
   "source": [
    "```{'eval_loss': 14.340641021728516, 'eval_model_preparation_time': 0.005, 'eval_runtime': 8.9796, 'eval_samples_per_second': 66.039, 'eval_steps_per_second': 4.232} before training```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "625bfed1-2c0f-46bf-9b03-34cb962e1579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 38/38 [00:29<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "generated_test = []\n",
    "for i in tqdm(range(0, len(test_dataset_preprocessed), batch_size)):\n",
    "    batch_inputs = torch.tensor(\n",
    "        test_dataset_preprocessed[i : i + batch_size][\"input_ids\"]\n",
    "    ).to(device)\n",
    "    att_mask = torch.tensor(\n",
    "        test_dataset_preprocessed[i : i + batch_size][\"attention_mask\"]\n",
    "    ).to(device)\n",
    "\n",
    "    batch_generated_ids = model.generate(\n",
    "        input_ids=batch_inputs,\n",
    "        attention_mask=att_mask,\n",
    "        max_length=128,\n",
    "    )\n",
    "    generated_test.append(batch_generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "19a3501e-4c3b-4e36-b365-cca1b8fe4b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "decoded_preds_test = [\n",
    "    tokenizer.batch_decode(g, skip_special_tokens=True) for g in generated_test\n",
    "]\n",
    "decoded_preds_test = list(itertools.chain.from_iterable(decoded_preds_test))\n",
    "\n",
    "labels_test = test_dataset_preprocessed[\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "90b1028e-b404-47ee-b6f7-94c28d1827b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.150084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.525069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        metric     value\n",
       "0  Exact Match  0.150084\n",
       "1     F1 Score  0.525069"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "from pandas import DataFrame\n",
    "\n",
    "em, f1 = compute_metrics(decoded_preds_test, labels_test)\n",
    "\n",
    "df = DataFrame()\n",
    "df[\"metric\"] = [\"Exact Match\", \"F1 Score\"]\n",
    "df[\"value\"] = [em, f1]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5a4680-00c6-484a-b31a-fb9506809c2f",
   "metadata": {},
   "source": [
    "# Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "433ef089-8428-4d8a-ab28-392174a546a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6636286973953247, 'eval_model_preparation_time': 0.003, 'eval_runtime': 82.5138, 'eval_samples_per_second': 69.855, 'eval_steps_per_second': 4.375}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate(eval_dataset=valid_dataset_preprocessed)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "23886731-481c-45dd-a018-17e746c7f5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 361/361 [02:30<00:00,  2.40it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "generated_valid = []\n",
    "for i in tqdm(range(0, len(valid_dataset_preprocessed), batch_size)):\n",
    "    batch_inputs = torch.tensor(\n",
    "        valid_dataset_preprocessed[i : i + batch_size][\"input_ids\"]\n",
    "    ).to(device)\n",
    "    att_mask = torch.tensor(\n",
    "        valid_dataset_preprocessed[i : i + batch_size][\"attention_mask\"]\n",
    "    ).to(device)\n",
    "\n",
    "    batch_generated_ids = model.generate(\n",
    "        input_ids=batch_inputs,\n",
    "        attention_mask=att_mask,\n",
    "        max_length=128,\n",
    "    )\n",
    "    generated_valid.append(batch_generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dfcb9330-462f-4c3f-bf3f-8cea69ce45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_preds_valid = [\n",
    "    tokenizer.batch_decode(g, skip_special_tokens=True) for g in generated_valid\n",
    "]\n",
    "decoded_preds_valid = list(itertools.chain.from_iterable(decoded_preds_valid))\n",
    "\n",
    "labels_valid = valid_dataset_preprocessed[\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b9924197-4c09-46ab-9ef5-cc445baa1958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exact Match</td>\n",
       "      <td>0.408223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>0.718381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        metric     value\n",
       "0  Exact Match  0.408223\n",
       "1     F1 Score  0.718381"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em, f1 = compute_metrics(decoded_preds_valid, labels_valid)\n",
    "\n",
    "df = DataFrame()\n",
    "df[\"metric\"] = [\"Exact Match\", \"F1 Score\"]\n",
    "df[\"value\"] = [em, f1]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872905dd-49d0-4da1-98e0-2c358b834baf",
   "metadata": {},
   "source": [
    "The results are much better on the validation set! It's more similar to the training dataset. The answers are more brief than in the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da009b-b9fa-453d-89be-17ece50d5562",
   "metadata": {},
   "source": [
    "# Compare results - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2269b835-f346-41b9-aee3-d2e2d0fea55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      "    Jakiej karze podlega armator, który wykonuje rybołówstwo morskie w polskich obszarach morskich, z naruszeniem przepisów ustawy?\n",
      "prediction:\n",
      "    do wysokości nieprzekraczającej pięćdziesięciokrotnego przeciętnego wynagrodzenia miesięcznego w gospodarce narodowej za rok poprzedzający, ogłaszanego przez Prezesa Głównego Urzędu Statystycznego\n",
      "actual_answer:\n",
      "    w zależności od wielkości łodzi podlega karze pieniężnej do wysokości nieprzekraczającej pięćdziesięciokrotnego przeciętnego wynagrodzenia miesięcznego w gospodarce narodowej za rok poprzedzający\n",
      "\n",
      "question:\n",
      "    Czy żołnierze przy wykonaniu czynności służbowej nie muszą się przedstawiać?\n",
      "prediction:\n",
      "    nie\n",
      "actual_answer:\n",
      "    eie umundurowany funkcjonariusz jest obowiązany na żądanie obywatela, wobec którego wykonuje czynność służbową, okazać legitymację służbową\n",
      "\n",
      "question:\n",
      "    W jakim przypadku policja może dokonać przeszukania pomieszczeń w domu?\n",
      "prediction:\n",
      "    w wypadkach nie cierpiących zwłoki\n",
      "actual_answer:\n",
      "    przeszukania zamieszkałych pomieszczeń można dokonać w  w wypadkach nie cierpiących zwłoki\n",
      "\n",
      "question:\n",
      "    Jakie obowiązki ma pracownik nadzorujący czynności kontrolne?\n",
      "prediction:\n",
      "    w tajemnicy informacje, które uzyskał w związku z wykonywaniem czynności służbowych\n",
      "actual_answer:\n",
      "    jest obowiązany zachować w tajemnicy informacje, które uzyskał w związku z wykonywaniem czynności służbowych\n",
      "\n",
      "question:\n",
      "    Co się stanie jeżeli nie zostanie uiszczona opłata dla wniosku patentowego?\n",
      "prediction:\n",
      "    nie\n",
      "actual_answer:\n",
      "    Urząd Patentowy wyda decyzję o odmowie udzielenia patentu.\n"
     ]
    }
   ],
   "source": [
    "idx_start = 30\n",
    "idx_end = 35\n",
    "\n",
    "for q, p, a in zip(\n",
    "    test_dataset_preprocessed[\"question\"][idx_start:idx_end],\n",
    "    decoded_preds_test[idx_start:idx_end],\n",
    "    labels_test[idx_start:idx_end],\n",
    "):\n",
    "    print(f\"question:\\n    {q}\")\n",
    "\n",
    "    print(f\"prediction:\\n    {p}\")\n",
    "\n",
    "    print(f\"actual_answer:\\n    {a}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bcbb6c-1404-4323-b9e1-1971d3c4b505",
   "metadata": {},
   "source": [
    "# Compare results - validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "39d54fcb-6384-47be-9d25-0fa366b45c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      "    Wynagrodzenie w jakiej wysokości otrzymała para za zezwolenie na opublikowanie relacji ze ślubu?\n",
      "prediction:\n",
      "    pięćset tysięcy funtów\n",
      "actual_answer:\n",
      "    pięćset tysięcy funtów\n",
      "\n",
      "question:\n",
      "    Dlaczego w zapusty ludzie spożywali więcej jedzenia?\n",
      "prediction:\n",
      "    tak\n",
      "actual_answer:\n",
      "    chciano zaspokoić głód przed zbliżającym się postem\n",
      "\n",
      "question:\n",
      "    Jakie dania jedzono w czasie karnawału? \n",
      "prediction:\n",
      "    placki ziemniaczane\n",
      "actual_answer:\n",
      "    tłuste\n",
      "\n",
      "question:\n",
      "    Czy w zapusty dozwolone było spożywanie mięsa?\n",
      "prediction:\n",
      "    nie\n",
      "actual_answer:\n",
      "    tak\n",
      "\n",
      "question:\n",
      "    Co oprócz placków ziemniaczanych smażono w czasie karnawału? \n",
      "prediction:\n",
      "    pączki, faworki i bliny\n",
      "actual_answer:\n",
      "    pączki, faworki i bliny\n"
     ]
    }
   ],
   "source": [
    "idx_start = 10\n",
    "idx_end = 15\n",
    "\n",
    "for q, p, a in zip(\n",
    "    valid_dataset_preprocessed[\"question\"][idx_start:idx_end],\n",
    "    decoded_preds_valid[idx_start:idx_end],\n",
    "    labels_valid[idx_start:idx_end],\n",
    "):\n",
    "    print(f\"question:\\n    {q}\")\n",
    "\n",
    "    print(f\"prediction:\\n    {p}\")\n",
    "\n",
    "    print(f\"actual_answer:\\n    {a}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127cd914-f6f7-418e-b203-4d70b7bfbace",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. Does the performance on the validation dataset reflects the performance on your test set?\n",
    "    - It doesn't. Even though the test dataset contains abstractive answers, the abstractive training/validation targets extracted from the json files are much shorter. For example \"nie\" instead of \"jest to zabronione\". Because of that the model learns to also give very short answers. The abstractive naswers prepared by students for the simple-legal-questions-pl dataset are much more complex than simply \"yes\"/\"no\".\n",
    "2. What are the outcomes of the model on your test questions? Are they satisfying? If not, what might be the reason\n",
    "   for that?\n",
    "    - The answers on the test dataset aren't perfect but we can see that model learned to find some of the information that we want. In some cases it gives the wrong answer. It's possible that using the larger model or training it for more epochs would improve the performance\n",
    "4. Why extractive question answering is not well suited for inflectional languages?\n",
    "    - An answer extracted for a qa task in inflectional language will very often, if not most of the time, in the incorrect form that does not match the form used to formulate the actual answer even if the extracted information is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda1cc7-9f36-4119-9612-384d983d62d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
